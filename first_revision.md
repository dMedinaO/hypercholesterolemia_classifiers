Thank you for inviting me to revise the manuscript “Development of a Predictive Model for the Early Detection of Maternal Supraphysiological Hypercholesterolemia” submitted to Acta Obstetrica et Gynecologica Scandinavica.

The study initially included a low number of observations with complete cases (n = 148). While the data is collected prospectively, women developing pregnancy complications later on (e.g. preeclampsia, gestational diabetes, fetal growth restriction or stillbirth) were excluded which resulted in a highly selected study population. Generalizability is not possible and a screening for the total pregnant population in early pregnancy, suggested by the authors, not justifiable.

It is not discussed how the low number of observations affects the results from the machine learning models, considering the statistical methods applied. Steps like data-preprocessing, dealing with imbalance and hyperparameter tuning are described, but not critically reflected. For feature scaling, max-absolute scaling is mentioned but not justified. Different scaling methods can significantly impact algorithm performance and should be evaluated systematically. The data set is initially imbalanced with a proportion of 29% positive cases (approximately 43 cases of maternal supraphysiological hypercholesterolemia (MSPH)). To balance the data the authors randomly under- sample the majority class (no MSPH) but do not justify this choice over other approaches (e.g. stratified sampling). Multiple balancing strategies should be compared and discussed. Additionally, it is not clearly stated how many observations are included in the final, balanced sample. With the chosen method, the number of observations could be even lower than the initial 148 complete cases. With a 90/10 split, the test set would comprise approximately 15 cases in the initial (non- balanced) study population which is insufficient for a reliable performance estimation. While hyperparameter tuning is conducted on the training set in "over 2,000 models" with "various hyperparameter configurations" the search strategy or validation methodology is not described in detail. It should be discussed how such an extensive hyperparameter tuning on a low number of observations could influence the results in terms of overfitting. Additionally, the risk of overfitting should also be highlighted regarding the number of features (n = 11) with respect to the size of the study population and learning curves showing performance vs. training set size should be provided. Beside the chosen performance metrics, clinical prediction models should report calibration plots showing whether predicted probabilities match observed frequencies. The paper does also not discuss how classification thresholds should be chosen for clinical use, considering different costs of false positives vs. false negatives. To identify the most important features, Shapley values are calculated, but there's no assessment of feature importance stability across cross-validation folds or bootstrap samples.

For the clinical implication of a machine learning model predicting an outcome further evaluation of the developed model is needed, for example by temporal validation. Since this study is meant to predict future outcomes, the authors should discuss whether time-based validation (earlier pregnancies in training, later ones in testing) would be more appropriate than random splitting. While code is available on GitHub, the paper should specify random seeds used for train-test splits and cross-validation to ensure reproducibility. Furthermore, for statistical analysis a library called DMAKit is used. This seems to be a tool which analysis data without previous statistical knowledge from the user. It should be carefully discussed how this tool was used and how that potentially influences the results and compare it with the literature.